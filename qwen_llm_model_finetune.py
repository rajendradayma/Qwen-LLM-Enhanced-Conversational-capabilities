# -*- coding: utf-8 -*-
"""Qwen llm model finetune.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hMhh30gXnDivBS2cnpXVLy9W8Et--Ip6
"""

!pip install -q transformers datasets accelerate sentencepiece pandas bitsandbytes peft evaluate einops scipy

pip install evaluate

from transformers import DataCollatorForLanguageModeling

pip install -U bitsandbytes

!pip install -U bitsandbytes
!pip install -U accelerate
!pip install -U transformers
!pip install -U peft
!pip install -U datasets
!pip install -U rouge-score

# [Imports]
import torch
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)

from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from datasets import Dataset
import pandas as pd
import ast
import evaluate
import os

# Device setup
device = "cuda" if torch.cuda.is_available() else "cpu"
torch.cuda.empty_cache() if device == "cuda" else None
print(f"Using device: {device}")

# Data loading + formatting
def prepare_data(file_path, sample_size=1500):
    df = pd.read_csv(file_path, nrows=sample_size)
    df["dialog"] = df["dialog"].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)

    def format_convo(dialog):
        conv = "<|im_start|>system\nYou are a helpful assistant<|im_end|>\n"
        for i, text in enumerate(dialog):
            role = "user" if i % 2 == 0 else "assistant"
            text = text[:150] + "..." if len(text) > 150 else text
            conv += f"<|im_start|>{role}\n{text.strip()}<|im_end|>\n"
        return conv

    texts = [format_convo(d) for d in df["dialog"]]
    return Dataset.from_dict({"text": texts})

train_dataset = prepare_data("train.csv", sample_size=500)

# Load base model/tokenizer
model_name = "Qwen/Qwen2.5-0.5B"
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    trust_remote_code=True,
    device_map="auto",
    torch_dtype=torch.float16,
    load_in_4bit=True

# Prepare model for PEFT + LoRA
model = prepare_model_for_kbit_training(model)

lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules = ["q_proj", "v_proj"],
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM"
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()

# Tokenization
def tokenize(examples):
    return tokenizer(
        examples["text"],
        truncation=True,
        max_length=256,
        padding="max_length"
    )

tokenized_train = train_dataset.map(tokenize, batched=True, remove_columns=["text"])
tokenized_train.set_format("torch", columns=["input_ids", "attention_mask"])

from transformers import DataCollatorForLanguageModeling

data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False
)


# Training Args
training_args = TrainingArguments(
    output_dir="./qwen2.5-lora-finetuned",
    num_train_epochs=10,
    per_device_train_batch_size=2,
    gradient_accumulation_steps=8,
    logging_steps=10,
    save_steps=200,
    learning_rate=2e-4,
    fp16=True,
    report_to="none",
    save_total_limit=1,
    gradient_checkpointing=True,
)

# Trainer Init
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    data_collator=data_collator
)

# Train
print("Training started...")
trainer.train()

import matplotlib.pyplot as plt

# Step numbers (every 10 steps)
steps = list(range(10, 330, 10))

# training loss values
losses = [
    4.7065, 4.0352, 3.5483, 3.1986, 3.1206, 2.9119, 2.9159, 2.8415, 2.7362, 2.6599,
    2.6806, 2.6923, 2.5650, 2.5379, 2.5079, 2.5145, 2.4355, 2.3950, 2.4350, 2.4868,
    2.3956, 2.3828, 2.3020, 2.3942, 2.3956, 2.4178, 2.3412, 2.3615, 2.3281, 2.3378,
    2.3546, 2.3505
]

# Plotting
plt.figure(figsize=(10, 5))
plt.plot(steps, losses, marker='o', linestyle='-', color='teal')
plt.title("Training Loss over Steps")
plt.xlabel("Training Step")
plt.ylabel("Loss")
plt.grid(True)
plt.xticks(steps, rotation=45)
plt.tight_layout()
plt.show()

import evaluate
rouge = evaluate.load("rouge")

predictions = ["The cat sat on the mat."]
references = ["A cat is sitting on the mat."]

rouge.add_batch(predictions=predictions, references=references)
print(rouge.compute())

import numpy as np
from tqdm import tqdm
import torch

def compute_perplexity(model, dataset, max_samples=100):
    model.eval()
    # Ensure model is on the correct device
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model.to(device)

    losses = []

    # Evaluate on a subset of the *tokenized* dataset
    subset = dataset.select(range(min(len(dataset), max_samples)))

    for i in tqdm(range(len(subset))):
        # Access tokenized data directly
        inputs = {k: v.unsqueeze(0).to(model.device) for k, v in subset[i].items()}

        with torch.no_grad():
            # Use the tokenized input_ids and attention_mask
            outputs = model(**inputs, labels=inputs['input_ids'])
            loss = outputs.loss.item()
            losses.append(loss)

    perplexity = np.exp(np.mean(losses))
    return perplexity

# Call it with the tokenized dataset
print("Perplexity:", compute_perplexity(model, tokenized_train))



peft_path = "./qwen2.5-lora-adapter"
model.save_pretrained(peft_path)
tokenizer.save_pretrained(peft_path)
print(f"\n LoRA adapter saved to {peft_path}")

# Save full model
full_model_path = "./qwen2.5-lora-full"
model.base_model.save_pretrained(full_model_path)
tokenizer.save_pretrained(full_model_path)
print(f" Full model with adapter saved to {full_model_path}")

# Human Evaluation Prompt
print("\n Human Evaluation Prompt:")
example = "<|im_start|>user\nHow do I improve my Python skills?<|im_end|>\n<|im_start|>assistant\n"
input_ids = tokenizer(example, return_tensors="pt").to(model.device)

output = model.generate(
    input_ids.input_ids,
    max_new_tokens=100,
    temperature=0.7,
    do_sample=True,
    pad_token_id=tokenizer.pad_token_id
)

generated = tokenizer.decode(output[0], skip_special_tokens=False)
response = generated.split("<|im_start|>assistant\n")[-1].split("<|im_end|>")[0]
print("\n Assistant's Response:\n", response)
print("\n Human Eval → Rate on: Relevance, Coherence, Helpfulness (1–5 stars)")

from transformers import pipeline
import torch

# List of user prompts for evaluation
prompts = [
    "How do I improve my Python skills?",
    "What is the future of AI in healthcare?",
    "Explain the concept of attention mechanism in transformers.",
    "How can I get started with machine learning?",
    "What are the best practices for clean code?"
]

# Prepare inputs
formatted_prompts = [
    f"<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\n" for prompt in prompts
]

# Generate responses
for i, prompt in enumerate(formatted_prompts):
    input_ids = tokenizer(prompt, return_tensors="pt").to(model.device)

    output = model.generate(
        input_ids.input_ids,
        max_new_tokens=150,
        temperature=0.7,
        do_sample=True,
        pad_token_id=tokenizer.pad_token_id
    )

    generated = tokenizer.decode(output[0], skip_special_tokens=False)
    response = generated.split("<|im_start|>assistant\n")[-1].split("<|im_end|>")[0]

    # Print for human evaluation
    print(f"\n Prompt {i+1}: {prompts[i]}")
    print(f" Assistant Response:\n{response}")
    print(" Human Eval → Rate on: Relevance, Coherence, Helpfulness (1–5 stars)\n")

import shutil

# Zip the LoRA adapter
shutil.make_archive('qwen2.5-lora-adapter', 'zip', './qwen2.5-lora-adapter')

# Zip the full model
shutil.make_archive('qwen2.5-lora-full', 'zip', './qwen2.5-lora-full')

from google.colab import files

# Download zipped LoRA adapter
files.download('qwen2.5-lora-adapter.zip')

# Download zipped full model
files.download('qwen2.5-lora-full.zip')